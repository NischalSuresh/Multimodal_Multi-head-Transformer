{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* Assuming 3 modalities  \n",
        "* All the modalities are processed in parallel\n",
        "* Drop out not added to check if output matches from both implementation\n",
        "* QKV in a single operation; change dim_head --> dim_head * 3; First split the different modes, then heads and then qkv\n",
        "* Head projection added\n",
        "* Mask added for 1st modality (In CLIP text encoding is masked attn while image encoding is not)\n",
        "* Different modes have different number of heads. 1st mode has 2 heads, 2nd has 4 heads and 3rd has 6 heads. All the modalities are encoded assuming to have max(heads) ~ 6 in this case, then in the last projection the unnecesssary heads are masked\n",
        "* Different embedding dimensions for different modes like in CLIP\n",
        "\n",
        "## Accounting for different Num of heads\n",
        "* Assume the total number of heads for each modality equal to the max of the heads of all modalities\n",
        "* Calucalte the encoding as usal but in the last stage where we do the head projectioon, mask out the unnecessary heads\n",
        "\n",
        "## Accounting for differernt Embedding Dim:\n",
        "* Assume the embedding dim to be the maximum emb dim of all the modalities\n",
        "* Zero out the additional embeddings where ever it is not necessary (Masking it later will not suffice). This is important since we do not want it to affect the softmax after the dot product. DUmbfuck--Its going to change the dot product itself\n",
        "* After calculating the scaled dot product attention, in the last projection there is no need to mask out the unnecessary embeddings from the value vector since k,q,v are already zero. Just slice the output accordingly\n"
      ],
      "metadata": {
        "id": "Xog57NJTxTGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t-Lr2fnFxRsr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YAVoKlijgWB",
        "outputId": "ec6d36e8-9dd4-4e2f-f94b-3c8d638bdebc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom linear layer to mask connections between different modalities. During the encoding the differernt modalities should be encoded independently"
      ],
      "metadata": {
        "id": "MD1hpWVXJfB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, mask = None, bias=False):\n",
        "        super().__init__()\n",
        "        #self.weight = nn.Parameter(torch.Tensor(out_features, in_features) * mask)\n",
        "        torch.manual_seed(0)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.weight = torch.ones(out_features, in_features).to(device)  #weights set to 1 to check if output matches, random seed not suitable becuase matrix size is different\n",
        "        # change this to random later\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.mask = mask\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.mask is None:\n",
        "          output = input.matmul(self.weight.t())\n",
        "        else:\n",
        "          self.mask = self.mask.to(device)\n",
        "          output = input.matmul((self.weight * self.mask).t()) # mask added to skip connections between different modalities, see notes\n",
        "        if self.bias is not None:\n",
        "            output += self.bias\n",
        "        return output"
      ],
      "metadata": {
        "id": "ZnwO1aczx_SF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard Multi head Attention (Dropout not added to remove randomizations to help verify output)"
      ],
      "metadata": {
        "id": "_LYtsRgkBsfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_head, num_heads, num_modes, ip_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_head = dim_head\n",
        "        self.ip_dim = ip_dim\n",
        "        self.embed_dim = self.num_heads * self.dim_head * num_modes\n",
        "        self.qkv_proj = CustomLinear(self.ip_dim * num_modes, self.embed_dim * 3)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None):\n",
        "        N, S, D = query.shape\n",
        "        N, T, D = value.shape\n",
        "        query = self.query_proj(query)\n",
        "        key = self.key_proj(key)\n",
        "        value = self.value_proj(value)\n",
        "        dot_product = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.embed_dim)\n",
        "        if attn_mask is not None:\n",
        "            additive_mask = (1 - attn_mask) * -1e9\n",
        "            dot_product += additive_mask\n",
        "        y = torch.matmul(dot_product, value)\n",
        "        return y\n",
        "\n",
        "class MultiHeadAttentionLayer(AttentionLayer):\n",
        "\n",
        "    def __init__(self, dim_head, num_heads, num_modes, ip_dim, dropout=0.1):\n",
        "        super().__init__(dim_head,num_heads, num_modes, ip_dim, dropout)\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_head = dim_head\n",
        "        self.num_modes = num_modes\n",
        "        self.embed_dim = self.num_heads * self.dim_head * self.num_modes\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.head_proj1 = CustomLinear(self.embed_dim, self.embed_dim *4)\n",
        "        self.head_proj2 = CustomLinear(self.embed_dim*4, ip_dim * num_modes)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        H = self.num_heads\n",
        "        N, S, D_ = x.shape\n",
        "        D = self.embed_dim * 3\n",
        "        qkv_proj = self.qkv_proj(x).view(N, S, 3*H, D // (3*H)).transpose(1,2)\n",
        "        query, key, value = torch.chunk(qkv_proj, 3, dim = 1)\n",
        "        dot_product = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dim_head)\n",
        "        if attn_mask is not None:\n",
        "            additive_mask = (1 - attn_mask) * -1e9\n",
        "            dot_product += additive_mask.to(query.device)\n",
        "        y = torch.matmul(F.softmax(dot_product, dim=-1), value)\n",
        "        output = y.transpose(1,2).reshape(N, S, D//3)\n",
        "        output = self.head_proj1(output)\n",
        "        output = self.head_proj2(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "LuwM3r6NofUD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### different number of heads and embedding dim for each modality"
      ],
      "metadata": {
        "id": "oSyr2dhFGoAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_attn1 = MultiHeadAttentionLayer(dim_head = 2, num_heads = 2, num_modes = 1, ip_dim = 5).to(device) #dim_head is the embedding dim per head\n",
        "multi_attn2 = MultiHeadAttentionLayer(dim_head = 4, num_heads = 4, num_modes = 1, ip_dim = 5).to(device)\n",
        "multi_attn3 = MultiHeadAttentionLayer(dim_head = 6, num_heads = 6, num_modes = 1, ip_dim = 5).to(device)"
      ],
      "metadata": {
        "id": "UcJH7QcTofEl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "x1 = torch.rand(2,4,5).to(device) # modality input one of Batch_size - 2, sequence length - 4, dim - 5\n",
        "x2 = torch.rand(2,4,5).to(device) # modality input two of Batch_size - 2, sequence length - 4, dim - 5\n",
        "x3 = torch.rand(2,4,5).to(device) # modality input three of Batch_size - 2, sequence length - 4, dim - 5\n",
        "attn_mask1 = torch.tril(torch.ones(4,4)).to(device)\n",
        "for _ in range(1):\n",
        "  x1 = multi_attn1(x1, attn_mask1) # generate the output from all the modalities one after the other # Shape --> [N x S x (Ip_dim * num_modes)]\n",
        "  x2 = multi_attn2(x2)\n",
        "  x3 = multi_attn3(x3)\n",
        "out = torch.cat((x1, x2, x3), -1)"
      ],
      "metadata": {
        "id": "N80liM97Ki2J"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFPDpLRyOXGr",
        "outputId": "2dfff6f0-d9a4-4fac-90b5-6bc9e0068077"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape # Shape --> [N x S x (Ip_dim * num_modes)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1OT0ywvGxrn",
        "outputId": "51e5f3b2-d1d0-4379-e5e2-7bbe804ef220"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Multi head attention with parallel processing of modalities\n",
        " * Shape of mask shoudl be (out_features x in_features) --> Check custom linear class"
      ],
      "metadata": {
        "id": "a4LT66eiVKYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FuseAttentionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_heads_list, num_heads_list, num_modes, ip_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = max(num_heads_list)\n",
        "        self.dim_heads = max(dim_heads_list)\n",
        "        self.dim_heads_list = dim_heads_list\n",
        "        self.ip_dim = ip_dim\n",
        "        #create first mask for input projection\n",
        "        a = self.num_heads * self.dim_heads * 3 # Multiply by 3 to account for qkv\n",
        "        b = ip_dim\n",
        "        B = b * num_modes\n",
        "        out =  torch.hstack((torch.ones(a,b), torch.zeros(a,B - b)))\n",
        "        mask = out\n",
        "        a1 = torch.tensor(num_heads_list) * torch.tensor(dim_heads_list)\n",
        "        b1 = a1 * 4\n",
        "        #create second mask for out projection and then to remove extra heads\n",
        "        # a1 = a//3 # a includes query, key and value\n",
        "        # out1 =  torch.hstack((torch.ones(a1,b), torch.zeros(a1,B - b)))\n",
        "        # mask1 = out1\n",
        "        for _ in range(num_modes-1):   # to generate the required mask, check notes --> perhaps simpler to hard code the matrix like mask1\n",
        "          out = torch.roll(out, shifts=b, dims=-1)\n",
        "          mask = torch.vstack((mask,out))\n",
        "        mask1_mode1 = torch.hstack((torch.ones(a1[0],b1[0]), torch.zeros(a1[0],b1[1]), torch.zeros(a1[0],b1[2])))\n",
        "        mask1_mode2 = torch.hstack((torch.zeros(a1[1],b1[0]), torch.ones(a1[1],b1[1]), torch.zeros(a1[1],b1[2])))\n",
        "        mask1_mode3 = torch.hstack((torch.zeros(a1[2],b1[0]), torch.zeros(a1[2],b1[1]), torch.ones(a1[2],b1[2])))\n",
        "        mask1 = torch.vstack((mask1_mode1, mask1_mode2, mask1_mode3)).t() # transpose to get the shape (output_dim x input_dim)\n",
        "        mask2_mode1 = torch.hstack((torch.ones(b1[0],ip_dim), torch.zeros(b1[0], ip_dim), torch.zeros(b1[0],ip_dim)))\n",
        "        mask2_mode2 = torch.hstack((torch.zeros(b1[1],ip_dim), torch.ones(b1[1], ip_dim), torch.zeros(b1[1],ip_dim)))\n",
        "        mask2_mode3 = torch.hstack((torch.zeros(b1[2],ip_dim), torch.zeros(b1[2], ip_dim), torch.ones(b1[2],ip_dim)))\n",
        "        mask2 = torch.vstack((mask2_mode1, mask2_mode2, mask2_mode3)).t() # transpose to get the shape (output_dim x input_dim)\n",
        "        # pdb.set_trace()\n",
        "        # out1 = torch.roll(out1, shifts=b, dims=-1)\n",
        "        # mask1 = torch.vstack((mask1,out1))\n",
        "        # heads_remove=[] # stores a list of rows to mask to ignore certains heads\n",
        "        # for i,j in enumerate(num_heads_list):\n",
        "        #   for k in range(max(num_heads_list)*self.dim_heads - j*self.dim_heads):\n",
        "        #     heads_remove.append(i*self.num_heads*self.dim_heads + (max(num_heads_list)*self.dim_heads-k-1))\n",
        "        # # pdb.set_trace()\n",
        "        # mask1[heads_remove] = 0\n",
        "        # #print(\"mask1\", mask1)\n",
        "        # # print(\"shape of mask =\", mask.shape)\n",
        "        # mask1 = mask1.t()\n",
        "        self.embed_dim = self.num_heads * self.dim_heads * num_modes\n",
        "        self.qkv_proj = CustomLinear(self.ip_dim * num_modes, self.embed_dim * 3, mask) # *3 is to account for qkv in the same proj\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        proj1_input = torch.tensor(num_heads_list) @ torch.tensor(dim_heads_list)\n",
        "        self.head_proj1 = CustomLinear(proj1_input, proj1_input * 4, mask1)\n",
        "        self.head_proj2 = CustomLinear(proj1_input * 4, ip_dim * num_modes, mask2)\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None):\n",
        "        N, S, D = query.shape\n",
        "        N, T, D = value.shape\n",
        "        query = self.query_proj(query)\n",
        "        key = self.key_proj(key)\n",
        "        value = self.value_proj(value)\n",
        "        dot_product = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.embed_dim)\n",
        "        if attn_mask is not None:\n",
        "            additive_mask = (1 - attn_mask) * -1e9\n",
        "            dot_product += additive_mask\n",
        "        y = torch.matmul(F.softmax(dot_product, dim=-1), value)\n",
        "        return y\n",
        "\n",
        "class FuseMultiHeadAttentionLayer(FuseAttentionLayer):\n",
        "\n",
        "    def __init__(self, dim_heads_list, num_heads_list, num_modes, ip_dim, dropout=0.1):\n",
        "        super().__init__(dim_heads_list,num_heads_list, num_modes, ip_dim, dropout)\n",
        "        self.num_heads = max(num_heads_list)\n",
        "        self.dim_heads = max(dim_heads_list)\n",
        "        self.num_modes = num_modes\n",
        "        self.embed_dim = self.num_heads * self.dim_heads * self.num_modes\n",
        "        self.dim_heads_list = dim_heads_list\n",
        "        self.num_heads_list = num_heads_list\n",
        "         # splice required values before head projection\n",
        "        heads_skip = torch.arange(3) * self.num_heads # create a tensor to get the start numbers for heads\n",
        "        heads_keep = torch.cat((torch.arange(heads_skip[0], heads_skip[0]+self.dim_heads_list[0]), torch.arange(heads_skip[1], heads_skip[1]+self.dim_heads_list[1]), torch.arange(heads_skip[2], heads_skip[2]+self.dim_heads_list[2])))\n",
        "        self.dim_heads_list = torch.tensor(self.dim_heads_list)\n",
        "        dims_heads_keep = torch.cat((self.dim_heads_list[0].repeat(self.num_heads_list[0]), self.dim_heads_list[1].repeat(self.num_heads_list[1]), self.dim_heads_list[2].repeat(self.num_heads_list[2])))\n",
        "        # print(heads_keep)\n",
        "        # print(dims_heads_keep)\n",
        "        dims_keep = [] # store the indices of the elements to keep based on the heads and emb_dim of each modality\n",
        "        # pdb.set_trace()\n",
        "        for i in range(len(heads_keep)):\n",
        "            values = torch.arange((heads_keep[i] * self.dim_heads),(heads_keep[i] * self.dim_heads + dims_heads_keep[i]))\n",
        "            dims_keep.append(values)\n",
        "        self.dims_keep = torch.cat(dims_keep)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        H = self.num_heads\n",
        "        N, S, D_ = x.shape\n",
        "        D = self.embed_dim * 3\n",
        "        M = self.num_modes\n",
        "        # query shape = Batch x seq_len x (ip_dim*Num_mode)\n",
        "        qkv_proj = self.qkv_proj(x) #shape - B x seq_len x emb_dim\n",
        "        qkv_proj = qkv_proj.view(N, S, M, D // M).transpose(-3,-2) #shape - B x N_modes x seq_len x (emb_dim/N_modes)\n",
        "        qkv_proj = qkv_proj.view(N, M, S, 3 * H, D // (M*H*3)).transpose(-3,-2) #shape - B x N_modes x (3*N_head) x seq_len x (emb_dim/(N_head * N_modes)\n",
        "        query, key, value = torch.chunk(qkv_proj, 3, dim = 2) #shape - B x N_modes x (1*N_head) x seq_len x (emb_dim/(N_head * N_modes)\n",
        "        for i,dim_heads in enumerate(self.dim_heads_list): # equating the additional values in embedding dimension to zero\n",
        "          query[:,i,:,:,dim_heads:] = 0\n",
        "          key[:,i,:,:,dim_heads:] = 0\n",
        "          value[:,i,:,:,dim_heads:] = 0\n",
        "        dot_product = torch.matmul(query, key.transpose(-2, -1)) # [/ math.sqrt(self.dim_head)] --> do this division after creating chunks\n",
        "        # print(dot_product.transpose(-1,-2).reshape(N, M, S, D//(M)).transpose(-1,-2).reshape(N, S, D))\n",
        "        # print(dot_product.shape)\n",
        "        # Split to add mask for one particular mode\n",
        "        mode1_dp, mode2_dp, mode3_dp  = torch.chunk(dot_product, 3, dim=1)\n",
        "        mode1_dp = mode1_dp / math.sqrt(self.dim_heads_list[0])\n",
        "        mode2_dp = mode2_dp / math.sqrt(self.dim_heads_list[1])\n",
        "        mode3_dp = mode3_dp / math.sqrt(self.dim_heads_list[2])\n",
        "        if attn_mask is not None:\n",
        "            additive_mask = (1 - attn_mask) * -1e9\n",
        "            mode1_dp += additive_mask.to(query.device)\n",
        "        dot_product = torch.cat((mode1_dp, mode2_dp, mode3_dp), dim = 1)\n",
        "        y = torch.matmul(F.softmax(dot_product, dim=-1), value) #B x N_modes x N_head x seq_len x (emb_dim/(N_head * N_modes)\n",
        "        output = y.transpose(-3,-2).reshape(N, M, S, D//(3*M)) # B x N_modes x seq_len x (emb_dim/N_modes)\n",
        "        output = output.transpose(-3,-2).reshape(N, S, D//3) # B x seq_len x emb_dim\n",
        "        # print(output.shape)\n",
        "        #pdb.set_trace()\n",
        "        # print(dims_keep)\n",
        "        output_spliced = output[:,:,self.dims_keep] # B x seq_len x (M1H1D1,M1H1D2,..M1H2D1,...M2H1D1...) ~ B x seq_len x dims_keep\n",
        "        # print(\"spliced_output shape = \", output_spliced.shape)\n",
        "        # intermediate dim == 4 x dim_head\n",
        "        # pdb.set_trace()\n",
        "        output_proj = self.head_proj1(output_spliced) # B x seq_len x (dims_keep * 4)\n",
        "        output = self.head_proj2(output_proj) # B x seq_len x (ip_dim*num_modes)\n",
        "        return output"
      ],
      "metadata": {
        "id": "oLBRmqR8VHVI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "x1 = torch.rand(2,4,5) # modality input one of Batch_size - 2, sequence length - 4, ip_dim - 5\n",
        "x2 = torch.rand(2,4,5) # modality input two of Batch_size - 2, sequence length - 4, ip_dim - 5\n",
        "x3 = torch.rand(2,4,5) # modality input three of Batch_size - 2, sequence length - 4, ip_dim - 5\n",
        "attn_mask = torch.tril(torch.ones(4,4)).to(device)\n",
        "num_heads_list = [2,4,6]\n",
        "dim_heads_list = [2,4,6]\n",
        "fuse_multi_attn = FuseMultiHeadAttentionLayer(dim_heads_list = [2,4,6], num_heads_list = [2,4,6], num_modes = 3, ip_dim = 5).to(device) #dim_head is the embedding dim per head #num_head is heads per mode\n",
        "x_cat = torch.cat((x1,x2,x3),-1).to(device)  # concatenate all the different modes together\n",
        "for _ in range(1):\n",
        "  x_cat = fuse_multi_attn(x_cat, attn_mask) # generate the output from the cancatenated input\n",
        "out_fuse = x_cat"
      ],
      "metadata": {
        "id": "FtWBKsZBBMMu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_fuse.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCJUbkFrmYET",
        "outputId": "bd4ed6c0-9937-468c-dd1c-3deb7cc9d7dc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(out_fuse, out) # check if the output from both the methods are the same"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXK0RNyIH9Ox",
        "outputId": "e80ce7bc-7160-4221-99a1-c5d60f94b0aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Comparasion"
      ],
      "metadata": {
        "id": "uHCoOL0_8TI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "4Qpbqlsb8o69"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "start_time1 = time.time()\n",
        "for _ in range(1000):\n",
        "  x1 = torch.rand(2,4,5).to(device)\n",
        "  x2 = torch.rand(2,4,5).to(device)\n",
        "  x3 = torch.rand(2,4,5).to(device)\n",
        "\n",
        "  out1 = multi_attn1(x1)\n",
        "  out2 = multi_attn2(x2)\n",
        "  out3 = multi_attn3(x3)\n",
        "\n",
        "  out = torch.cat((out1, out2, out3), -1)\n",
        "end_time1 = time.time()\n",
        "time_1 = end_time1 - start_time1"
      ],
      "metadata": {
        "id": "Sc9rxHAL8S6e"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "start_time2 = time.time()\n",
        "for _ in range(1000):\n",
        "  x1 = torch.rand(2,4,5)\n",
        "  x2 = torch.rand(2,4,5)\n",
        "  x3 = torch.rand(2,4,5)\n",
        "\n",
        "  x_cat = torch.cat((x1,x2,x3),-1).to(device)\n",
        "  out_fuse = fuse_multi_attn(x_cat)\n",
        "end_time2 = time.time()\n",
        "time_2 = end_time2 - start_time2"
      ],
      "metadata": {
        "id": "sqemBB0l1A7h"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_std_multihead = round(time_1,2)\n",
        "time_my_multihead = round(time_2,2)"
      ],
      "metadata": {
        "id": "5wjpf0Y9IlYA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_std_multihead # time for standard implementation"
      ],
      "metadata": {
        "id": "7JRTT_Mu9rsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a50623-5e99-4926-8122-c10b39dc73ed"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.95"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_my_multihead # time for modified implementation"
      ],
      "metadata": {
        "id": "N4ki_k299yWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062a727e-93fa-40ec-fabd-3ad6f8e09157"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.07"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_std_multihead/time_my_multihead # can be upto 3 times as fast since there are 3 modalities. The time complexity scales linearly with modes in standard implementation but it is constant is the modified implementation"
      ],
      "metadata": {
        "id": "V8F5ltFx90IZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e29e589-fcd8-4523-9d92-bb22806a8b50"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1090458488228003"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m80xvxxDdVOr"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}