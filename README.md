# Multimodal_Multi-head-Transformer

Stack multiple multi-head attention blocks into one single block which is capable of processing different modalities parallely and give the exact same output as that of standard multihead attention block but at a much faster rate. The architecture is optimised to perform the encoding faster without any loss of accuracy.

The code is implemented in qkv_diffEmb_fiffHead_Fusion_multi_attn.ipynb
