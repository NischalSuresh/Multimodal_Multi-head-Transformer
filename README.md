# Multimodal_Multi-head-Transformer

Stack multiple multi-head attention blocks into one single block which is capable of processing different modalities parallely and give the exact same output as that of standard multihead attention block but at a much faster rate
