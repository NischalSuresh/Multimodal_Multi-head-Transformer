# Multimodal_Multi-head-Transformer

Stack multiple multi-head attention blocks into one single block which is capable of processing different modalities parallely and give the exact same output as that of standard multihead attention block but at a much faster rate

The code is implemented in qkv_diffEmb_fiffHead_Fusion_multi_attn.ipynb
