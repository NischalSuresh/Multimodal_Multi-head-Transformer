# Multimodal_Multi-head-Transformer

Stack multiple multi-head attention blocks into one single block which is capable of processing different modalities parallelly and gives the exact same output as that of a standard multi-head attention block but at a much faster rate. The architecture is optimized to perform the encoding faster without any loss of accuracy.

**The code is implemented in qkv_diffEmb_diffHead_Fusion_multi_attn.ipynb**
